---
title: "Overview — What is Supermemory?"
sidebarTitle: "Overview"
description = "Add long-term memory to your LLMs with three integration paths: AI SDK, Memory API, or Memory Router."
---

Supermemory gives your LLMs long-term memory. Instead of stateless text generation, they recall the right facts from your files, chats, and tools, so responses stay consistent, contextual, and personal.


## How does it work? (at a glance)

![](/images/232.png)

- You send Supermemory text, files, and chats.
- Supermemory [intelligently indexes them](/how-it-works) and builds a semantic understanding graph on top of an entity (e.g., a user, a document, a project, an organization).
- At query time, we fetch only the most relevant context and pass it to your models.

## Supermemory is context engineering.

#### Ingestion and Extraction

Supermemory handles all the extraction, for any data type that you have.
- Text
- Conversations
- Files (PDF, Images, Docs)
- Even videos!

... and then,

We offer three ways to add context to your LLMs:

#### Memory API — Learned user context

![memory graph](/images/memory-graph.png)

Supermemory learns and builds the memory for the user. These are extracted facts about the user, that:
- Evolve on top of existing context about the user, **in real time**
- Handle **knowledge updates, temporal changes, forgetfulness**
- Creates a **user profile** as the default context provider for the LLM.

_This can then be provided to the LLM, to give more contextual, personalized responses._

#### User profiles

Having the latest, evolving context about the user allows us to also create a **User Profile**. This is a combination of static and dynamic facts about the user, that the agent should **always know**
Developers can configure supermemory with what static and dynamic contents are, depending on their use case.

- Static: Information that the agent should **always** know.
- Dynamic: **Episodic** information, about last few conversations etc.

This leads to a much better retrieval system, and extremely personalized responses.

#### RAG - Advanced semantic search

Along with the user context, developers can also choose to do a search on the raw context. We provide full RAG-as-a-service, along with
- Full advanced metadata filtering
- Contextual chunking
- Works well with the memory engine

<Info>
    You can reference the full API reference for the Memory API [here](/api-reference/manage-documents/add-document).
</Info>


<Note>
All three approaches share the **same context pool** when using the same user ID (`containerTag`). You can mix and match based on your needs.
</Note>

## Next steps

Head to the [**How it works**](/how-it-works) guide to understand the underlying way of how supermemory represents and learns in data.
